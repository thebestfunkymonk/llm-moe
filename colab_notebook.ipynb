## ðŸ“¦ Setup and Dependencies# ðŸ§  LLM Research: Transformer vs Mixture of Experts

This notebook compares standard Transformer and MoE architectures for language modeling.

**Features:**
- Regular Transformer vs MoE (8 experts)
- Muon Optimizer (novel matrix-based optimizer)
- RoPE Positional Embeddings
- Automatic Mixed Precision
- Real-time training monitoring